{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae4ee9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (0.10.17)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.2.0,>=0.1.4 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index) (0.1.5)\n",
      "Requirement already satisfied: llama-index-cli<0.2.0,>=0.1.2 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index) (0.1.8)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.17 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index) (0.10.17)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.2.0,>=0.1.5 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index) (0.1.6)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index) (0.1.3)\n",
      "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index) (0.9.48)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.5 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index) (0.1.7)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index) (0.1.4)\n",
      "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.3 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index) (0.1.4)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.2 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index) (0.1.3)\n",
      "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.4 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index) (0.1.8)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse<0.2.0,>=0.1.2 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index) (0.1.3)\n",
      "Requirement already satisfied: llama-index-vector-stores-chroma<0.2.0,>=0.1.1 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.1.5)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.17->llama-index) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.17->llama-index) (2.0.28)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.17->llama-index) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.17->llama-index) (0.6.4)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.17->llama-index) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.17->llama-index) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.17->llama-index) (2024.2.0)\n",
      "Requirement already satisfied: httpx in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.17->llama-index) (0.27.0)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.13 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.17->llama-index) (0.1.13)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.17->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.17->llama-index) (3.2.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.17->llama-index) (3.8.1)\n",
      "Requirement already satisfied: numpy in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.17->llama-index) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.17->llama-index) (1.13.3)\n",
      "Requirement already satisfied: pandas in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.17->llama-index) (2.2.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.17->llama-index) (10.2.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.17->llama-index) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.17->llama-index) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.17->llama-index) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.17->llama-index) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.17->llama-index) (4.10.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.17->llama-index) (0.9.0)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.12.3)\n",
      "Requirement already satisfied: bs4<0.0.3,>=0.0.2 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (0.0.2)\n",
      "Requirement already satisfied: pymupdf<2.0.0,>=1.23.21 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (1.23.26)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.1.0)\n",
      "Requirement already satisfied: llama-parse<0.4.0,>=0.3.3 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index) (0.3.7)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.17->llama-index) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.17->llama-index) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.17->llama-index) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.17->llama-index) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.17->llama-index) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.17->llama-index) (4.0.3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: soupsieve>1.2 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (2.5)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.17->llama-index) (1.16.0)\n",
      "Requirement already satisfied: chromadb<0.5.0,>=0.4.22 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.4.24)\n",
      "Requirement already satisfied: onnxruntime<2.0.0,>=1.17.0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.17.1)\n",
      "Requirement already satisfied: tokenizers<0.16.0,>=0.15.1 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.15.2)\n",
      "Requirement already satisfied: pydantic>=1.10 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.17->llama-index) (2.6.3)\n",
      "Requirement already satisfied: anyio in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.17->llama-index) (4.3.0)\n",
      "Requirement already satisfied: certifi in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.17->llama-index) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.17->llama-index) (1.0.4)\n",
      "Requirement already satisfied: idna in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.17->llama-index) (3.6)\n",
      "Requirement already satisfied: sniffio in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.17->llama-index) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.17->llama-index) (0.14.0)\n",
      "Requirement already satisfied: click in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.17->llama-index) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.17->llama-index) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.17->llama-index) (2023.12.25)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.17->llama-index) (1.9.0)\n",
      "Requirement already satisfied: PyMuPDFb==1.23.22 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from pymupdf<2.0.0,>=1.23.21->llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (1.23.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.17->llama-index) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.17->llama-index) (1.26.18)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.17->llama-index) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.17->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.17->llama-index) (3.21.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.17->llama-index) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.17->llama-index) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.17->llama-index) (2024.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.17->llama-index) (1.2.0)\n",
      "Requirement already satisfied: build>=1.0.3 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.1.1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.7.3)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.110.0)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.27.1)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (3.5.0)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (3.4.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.44b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.23.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (6.1.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.62.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (4.1.2)\n",
      "Requirement already satisfied: typer>=0.9.0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.9.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (29.0.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (4.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (3.9.15)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: packaging>=17.0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.17->llama-index) (23.2)\n",
      "Requirement already satisfied: coloredlogs in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from onnxruntime<2.0.0,>=1.17.0->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from onnxruntime<2.0.0,>=1.17.0->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (24.3.6)\n",
      "Requirement already satisfied: protobuf in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from onnxruntime<2.0.0,>=1.17.0->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (4.25.3)\n",
      "Requirement already satisfied: sympy in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from onnxruntime<2.0.0,>=1.17.0->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.12)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.17->llama-index) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.17->llama-index) (2.16.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.17->llama-index) (1.16.0)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from tokenizers<0.16.0,>=0.15.1->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.21.4)\n",
      "Requirement already satisfied: pyproject_hooks in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from build>=1.0.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from build>=1.0.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (6.11.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from build>=1.0.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (2.0.1)\n",
      "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from fastapi>=0.95.2->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.36.3)\n",
      "Requirement already satisfied: filelock in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<0.16.0,>=0.15.1->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (3.13.1)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (2.28.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.7.0)\n",
      "Requirement already satisfied: requests-oauthlib in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.3.1)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (3.2.2)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.62.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.23.0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.23.0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.44b0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.44b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.44b0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.44b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.44b0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.44b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.44b0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.44b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from opentelemetry-instrumentation==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (69.1.1)\n",
      "Requirement already satisfied: asgiref~=3.0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from opentelemetry-instrumentation-asgi==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (3.7.2)\n",
      "Requirement already satisfied: monotonic>=1.5 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (2.2.1)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (12.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: humanfriendly>=9.1 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from coloredlogs->onnxruntime<2.0.0,>=1.17.0->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (10.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from importlib-resources->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (3.17.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from sympy->onnxruntime<2.0.0,>=1.17.0->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (4.9)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/xue/anaconda3/envs/llama.cpp/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.5.1)\n",
      "Requirement already satisfied: llama-index-llms-llama-cpp in /home/xue/anaconda3/lib/python3.11/site-packages (0.1.3)\n",
      "Requirement already satisfied: llama-cpp-python<0.3.0,>=0.2.32 in /home/xue/anaconda3/lib/python3.11/site-packages (from llama-index-llms-llama-cpp) (0.2.55)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /home/xue/anaconda3/lib/python3.11/site-packages (from llama-index-llms-llama-cpp) (0.10.17)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/xue/anaconda3/lib/python3.11/site-packages (from llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp) (4.9.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/xue/anaconda3/lib/python3.11/site-packages (from llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp) (1.24.4)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /home/xue/anaconda3/lib/python3.11/site-packages (from llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /home/xue/anaconda3/lib/python3.11/site-packages (from llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp) (3.1.2)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /home/xue/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /home/xue/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2.0.28)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /home/xue/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json in /home/xue/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (0.6.4)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /home/xue/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /home/xue/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/xue/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2023.10.0)\n",
      "Requirement already satisfied: httpx in /home/xue/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (0.26.0)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.13 in /home/xue/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (0.1.13)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/xue/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /home/xue/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (3.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /home/xue/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (3.8.1)\n",
      "Requirement already satisfied: openai>=1.1.0 in /home/xue/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.3.3)\n",
      "Requirement already satisfied: pandas in /home/xue/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2.0.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /home/xue/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (9.4.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /home/xue/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /home/xue/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (8.2.2)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /home/xue/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /home/xue/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (4.66.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /home/xue/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (0.9.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/xue/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/xue/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/xue/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/xue/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/xue/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.8.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/xue/anaconda3/lib/python3.11/site-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.14.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/xue/anaconda3/lib/python3.11/site-packages (from jinja2>=2.11.3->llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp) (2.1.1)\n",
      "Requirement already satisfied: pydantic>=1.10 in /home/xue/anaconda3/lib/python3.11/site-packages (from llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2.6.1)\n",
      "Requirement already satisfied: anyio in /home/xue/anaconda3/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (3.5.0)\n",
      "Requirement already satisfied: certifi in /home/xue/anaconda3/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/xue/anaconda3/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.0.3)\n",
      "Requirement already satisfied: idna in /home/xue/anaconda3/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (3.4)\n",
      "Requirement already satisfied: sniffio in /home/xue/anaconda3/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.2.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/xue/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (0.14.0)\n",
      "Requirement already satisfied: click in /home/xue/anaconda3/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (8.0.4)\n",
      "Requirement already satisfied: joblib in /home/xue/anaconda3/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/xue/anaconda3/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2022.7.9)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/xue/anaconda3/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.9.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/xue/anaconda3/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/xue/anaconda3/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.26.16)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/xue/anaconda3/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2.0.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/xue/anaconda3/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/xue/anaconda3/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (3.21.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/xue/anaconda3/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/xue/anaconda3/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/xue/anaconda3/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2023.3)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/xue/anaconda3/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (23.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/xue/anaconda3/lib/python3.11/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in /home/xue/anaconda3/lib/python3.11/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2.16.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/xue/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index\n",
    "%pip install llama-index-llms-llama-cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b388893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9bac545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.llms.llama_cpp.llama_utils import (\n",
    "    messages_to_prompt,\n",
    "    completion_to_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b33baba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/xue/Documents/UOB_master_project/llama.cpp/models/Llama-7B/ggml-model-f16.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9f84ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/xue/Documents/UOB_master_project/llama.cpp/models/Llama-7B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size = 12853.02 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3900\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1950.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1950.00 MiB, K (f16):  975.00 MiB, V (f16):  975.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    16.65 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   275.75 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\\\n' + system_message + '\\\\n<</SYS>>\\\\n\\\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'tokenizer.ggml.model': 'llama', 'general.file_type': '1'}\n",
      "Using gguf chat template: {% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\n",
      "Using chat eos_token: </s>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCPP(\n",
    "    model_path=model_path,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=512,\n",
    "    context_window=3900,\n",
    "    generate_kwargs={},\n",
    "    # set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": 1},\n",
    "    # transform inputs into Llama2 format\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bcdf5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    4357.26 ms\n",
      "llama_print_timings:      sample time =      33.04 ms /   188 runs   (    0.18 ms per token,  5689.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4356.69 ms /    79 tokens (   55.15 ms per token,    18.13 tokens per second)\n",
      "llama_print_timings:        eval time =   58535.39 ms /   187 runs   (  313.02 ms per token,     3.19 tokens per second)\n",
      "llama_print_timings:       total time =   63192.26 ms /   266 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Of course! Here is a poem about cats and dogs:\n",
      "\n",
      "Cats and dogs, so furry and bright,\n",
      "Bringing joy to our lives with their playful might.\n",
      "With tails that swish and paws that pounce,\n",
      "They chase each other 'round the room with a gentleounce.\n",
      "\n",
      "Their eyes so wide with mischief and glee,\n",
      "As they frolic and play with abandonment in the sea.\n",
      "With nary a care in the world to weigh them down,\n",
      "They bask in the love that surrounds them, never frowning.\n",
      "\n",
      "So here's to cats and dogs, our loyal friends,\n",
      "Bringing laughter and joy until the very end.\n",
      "For they know how to live life to the fullest,\n",
      "And fill our hearts with happiness, never leaving us hollow.\n"
     ]
    }
   ],
   "source": [
    "response = llm.complete(\"Hello! Can you tell me a poem about cats and dogs?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa39be17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Great! Based on your observation of the objects in the house, if you're looking for a gas stove, you should search in the kitchen area. This is because the kitchen is where you would typically find a gas stove in a household setting. Therefore, it makes sense to search in this area first."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   10842.58 ms\n",
      "llama_print_timings:      sample time =      13.74 ms /    66 runs   (    0.21 ms per token,  4803.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10842.30 ms /   119 tokens (   91.11 ms per token,    10.98 tokens per second)\n",
      "llama_print_timings:        eval time =   20909.08 ms /    65 runs   (  321.68 ms per token,     3.11 tokens per second)\n",
      "llama_print_timings:       total time =   31951.49 ms /   184 tokens\n"
     ]
    }
   ],
   "source": [
    "response_iter = llm.stream_complete(\"I observe the following clusters of objects while exploring a house: 1. couch. 2. wooden chair. 3. refrigerator. Where should I search next if I am looking for a gas stove? You should always provide justification \")\n",
    "for response in response_iter:\n",
    "    print(response.delta, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8bf2d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript of a dialog, where the User interacts with an Robot Assistant named Tom. This Tom bot assistant helps Users extract locations from conversations. Tom is helpful, kind, honest, good at writing, and never fails to answer the User's requests immediately and with precision.\n",
      "\n",
      "User: Hello, Tom.\n",
      "Bob: Hello. How may I help you today?\n",
      "User: go to the kitchen and then go to the toilet.\n",
      "Tom: kitchen, toilet.\n",
      "User: go to the chair and then go to another chair.\n",
      "Tom: chair, chair.\n",
      "User: navigate to the green sofa and turn right and find several chairs, finally go to the painting.\n",
      "Tom: green sofa, chairs, painting.\n",
      "User: approach the window in front, turn right and go to the television, and finally go by the oven in the kitchen.\n",
      "Tom: window, television, oven, kitchen.\n",
      "User: walk to the plant first, turn around and come back to the table, go further into the bedroom, and stand next to the bed.\n",
      "Tom: plant, table, bedroom, bed.\n",
      "User: go by the stairs, go to the room next to it, approach the book shelf and then go to the table in the next room.\n",
      "Tom: stairs, room, book shelf, table, next room.\n",
      "User: Go front left and move to the table, then turn around and find a cushion, later stand next to a column before finally navigate to any appliances.\n",
      "Tom: table, cushion, column, appliances.\n",
      "User: Move to the west of the chair, with the sofa on your right, move to the table, then turn right 90 degree, then find a table.\n",
      "Tom: chair, table.\n",
      "User: Move the robot from the Bed to the Dining Table in the room.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = '/home/xue/Documents/UOB_master_project/llama.cpp/prompts/chat-with-robot-v0.txt'  # 替换为您的文件路径\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c054dbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    4357.26 ms\n",
      "llama_print_timings:      sample time =      85.84 ms /   511 runs   (    0.17 ms per token,  5952.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  164845.73 ms /   511 runs   (  322.59 ms per token,     3.10 tokens per second)\n",
      "llama_print_timings:       total time =  165778.90 ms /   512 tokens\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.base.llms.types import ChatMessage\n",
    "\n",
    "messages = [ChatMessage(role=\"system\", content=content)]\n",
    "\n",
    "chat_response = llm.chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d368539d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Of course! I'm happy to assist you with that. To move the robot from the Bed to the Dining Table in the room, please provide me with specific directions or instructions on how to navigate through the space. I will do my best to follow your instructions to ensure a safe and efficient movement of the robot. Please let me know if you have any further questions or concerns!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     341.75 ms\n",
      "llama_print_timings:      sample time =      14.35 ms /    80 runs   (    0.18 ms per token,  5574.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4328.16 ms /    79 tokens (   54.79 ms per token,    18.25 tokens per second)\n",
      "llama_print_timings:        eval time =   25048.34 ms /    79 runs   (  317.07 ms per token,     3.15 tokens per second)\n",
      "llama_print_timings:       total time =   29548.02 ms /   158 tokens\n"
     ]
    }
   ],
   "source": [
    "input_message = \"Move the robot from the Bed to the Dining Table in the room.\"\n",
    "response_iter = llm.stream_complete(input_message)\n",
    "for response in response_iter:\n",
    "    print(response.delta, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191aee31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dac07dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/xue/Documents/UOB_master_project/llama.cpp/models/Llama-7B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size = 12853.02 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    10.01 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    70.50 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\\\n' + system_message + '\\\\n<</SYS>>\\\\n\\\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'tokenizer.ggml.model': 'llama', 'general.file_type': '1'}\n",
      "Using gguf chat template: {% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\n",
      "Using chat eos_token: </s>\n",
      "Using chat bos_token: <s>\n",
      "\n",
      "llama_print_timings:        load time =    2241.49 ms\n",
      "llama_print_timings:      sample time =       5.30 ms /    31 runs   (    0.17 ms per token,  5846.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2241.45 ms /    40 tokens (   56.04 ms per token,    17.85 tokens per second)\n",
      "llama_print_timings:        eval time =   10431.49 ms /    30 runs   (  347.72 ms per token,     2.88 tokens per second)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_print_timings:       total time =   12723.54 ms /    70 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-64248979-2a8e-4bf3-adcd-b49a03920e50', 'object': 'text_completion', 'created': 1709897416, 'model': '/home/xue/Documents/UOB_master_project/llama.cpp/models/Llama-7B/ggml-model-f16.gguf', 'choices': [{'text': '<s>[INST] <<SYS>>\\nYou are a helpful assistant\\n<</SYS>>\\nQ: Name the planets in the solar system? A:  [/INST]  Of course! Here are the names of the planets in our solar system, listed in order from closest to farthest from the Sun:', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 40, 'completion_tokens': 31, 'total_tokens': 71}}\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "model_path = \"/home/xue/Documents/UOB_master_project/llama.cpp/models/Llama-7B/ggml-model-f16.gguf\"\n",
    "llm = Llama(model_path=model_path)\n",
    "\n",
    "system_message = \"You are a helpful assistant\"\n",
    "user_message = \"Q: Name the planets in the solar system? A: \"\n",
    "\n",
    "prompt = f\"\"\"<s>[INST] <<SYS>>\n",
    "{system_message}\n",
    "<</SYS>>\n",
    "{user_message} [/INST]\"\"\"\n",
    "\n",
    "output = llm(\n",
    "  prompt, \n",
    "  max_tokens=32, \n",
    "  stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "  echo=True\n",
    ")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1fb1352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "You are a helpful assistant\n",
      "<</SYS>>\n",
      "Q: Name the planets in the solar system? A:  [/INST]  Of course! Here are the names of the planets in our solar system, listed in order from closest to farthest from the Sun:\n"
     ]
    }
   ],
   "source": [
    "print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "696be450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Obtaining dependency information for langchain from https://files.pythonhosted.org/packages/cb/cc/684ac2410ad06957c17ffda787b7ff6c1595721c36c3b443c021e95cf998/langchain-0.1.11-py3-none-any.whl.metadata\n",
      "  Downloading langchain-0.1.11-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/xue/anaconda3/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/xue/anaconda3/lib/python3.11/site-packages (from langchain) (2.0.28)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/xue/anaconda3/lib/python3.11/site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/xue/anaconda3/lib/python3.11/site-packages (from langchain) (0.6.4)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
      "  Obtaining dependency information for jsonpatch<2.0,>=1.33 from https://files.pythonhosted.org/packages/73/07/02e16ed01e04a374e644b575638ec7987ae846d25ad97bcc9945a3ee4b0e/jsonpatch-1.33-py2.py3-none-any.whl.metadata\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.25 (from langchain)\n",
      "  Obtaining dependency information for langchain-community<0.1,>=0.0.25 from https://files.pythonhosted.org/packages/8d/cc/387b93205020d23151c039e73805062c749a452a417fc578c7ea69efd469/langchain_community-0.0.27-py3-none-any.whl.metadata\n",
      "  Downloading langchain_community-0.0.27-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting langchain-core<0.2,>=0.1.29 (from langchain)\n",
      "  Obtaining dependency information for langchain-core<0.2,>=0.1.29 from https://files.pythonhosted.org/packages/53/b3/ae022560a8b104525b4ac1a97a557e3aa05dd0d233bb5284f7c63509742f/langchain_core-0.1.30-py3-none-any.whl.metadata\n",
      "  Downloading langchain_core-0.1.30-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
      "  Obtaining dependency information for langchain-text-splitters<0.1,>=0.0.1 from https://files.pythonhosted.org/packages/9d/a1/aec824080111e9b4a4802b51b988032faa193828c865e11233d1b18e88fa/langchain_text_splitters-0.0.1-py3-none-any.whl.metadata\n",
      "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Obtaining dependency information for langsmith<0.2.0,>=0.1.17 from https://files.pythonhosted.org/packages/dd/ec/562d602617ed75b45644cd6f40941171da2fc3ad08420763ef74178e9662/langsmith-0.1.23-py3-none-any.whl.metadata\n",
      "  Downloading langsmith-0.1.23-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/xue/anaconda3/lib/python3.11/site-packages (from langchain) (1.24.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/xue/anaconda3/lib/python3.11/site-packages (from langchain) (2.6.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/xue/anaconda3/lib/python3.11/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/xue/anaconda3/lib/python3.11/site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/xue/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/xue/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/xue/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/xue/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/xue/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/xue/anaconda3/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/xue/anaconda3/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/xue/anaconda3/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.1)\n",
      "Requirement already satisfied: anyio<5,>=3 in /home/xue/anaconda3/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.29->langchain) (3.5.0)\n",
      "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2,>=0.1.29->langchain)\n",
      "  Obtaining dependency information for packaging<24.0,>=23.2 from https://files.pythonhosted.org/packages/ec/1a/610693ac4ee14fcdf2d9bf3c493370e4f2ef7ae2e19217d7a237ff42367d/packaging-23.2-py3-none-any.whl.metadata\n",
      "  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/xue/anaconda3/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.9.14)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/xue/anaconda3/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in /home/xue/anaconda3/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (2.16.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/xue/anaconda3/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/xue/anaconda3/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/xue/anaconda3/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/xue/anaconda3/lib/python3.11/site-packages (from requests<3,>=2->langchain) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/xue/anaconda3/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/xue/anaconda3/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/xue/anaconda3/lib/python3.11/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.29->langchain) (1.2.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/xue/anaconda3/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Downloading langchain-0.1.11-py3-none-any.whl (807 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.5/807.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_community-0.0.27-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.1.30-py3-none-any.whl (256 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.9/256.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
      "Downloading langsmith-0.1.23-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Installing collected packages: packaging, jsonpatch, langsmith, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.1\n",
      "    Uninstalling packaging-23.1:\n",
      "      Successfully uninstalled packaging-23.1\n",
      "  Attempting uninstall: jsonpatch\n",
      "    Found existing installation: jsonpatch 1.32\n",
      "    Uninstalling jsonpatch-1.32:\n",
      "      Successfully uninstalled jsonpatch-1.32\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.4.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.4.3 requires pyqtwebengine<5.16, which is not installed.\n",
      "tables 3.8.0 requires blosc2~=2.0.0, which is not installed.\n",
      "tables 3.8.0 requires cython>=0.29.21, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed jsonpatch-1.33 langchain-0.1.11 langchain-community-0.0.27 langchain-core-0.1.30 langchain-text-splitters-0.0.1 langsmith-0.1.23 packaging-23.2\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cffb8e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baa6e413",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "Act as an Astronomer engineer who is teaching high school students.\n",
    "<</SYS>>\n",
    " \n",
    "{text} [/INST]\n",
    "\"\"\"\n",
    " \n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13a5d5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<s>[INST] <<SYS>>\n",
      "Act as an Astronomer engineer who is teaching high school students.\n",
      "<</SYS>>\n",
      " \n",
      "Explain what is the solar system in 2-3 sentences [/INST]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"Explain what is the solar system in 2-3 sentences\"\n",
    "print(prompt.format(text=text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "544fdfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints each new token\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c359015a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/xue/Documents/UOB_master_project/llama.cpp/models/Llama-7B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size = 12853.02 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     0.14 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     1.10 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\\\n' + system_message + '\\\\n<</SYS>>\\\\n\\\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'tokenizer.ggml.model': 'llama', 'general.file_type': '1'}\n",
      "Using gguf chat template: {% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\n",
      "Using chat eos_token: </s>\n",
      "Using chat bos_token: <s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ah, a great question! The solar system, my young apprentices, is a vast and fascinating expanse of celestial bodies that revolve around our very own star, the Sun. It consists of eight planets: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune, each with its unique characteristics and features. Together, they form a complex and intricate system that has captivated the imagination of astronomers and scientists for centuries."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     551.70 ms\n",
      "llama_print_timings:      sample time =      19.29 ms /   110 runs   (    0.18 ms per token,  5702.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3215.97 ms /    52 tokens (   61.85 ms per token,    16.17 tokens per second)\n",
      "llama_print_timings:        eval time =   33584.64 ms /   109 runs   (  308.12 ms per token,     3.25 tokens per second)\n",
      "llama_print_timings:       total time =   37029.52 ms /   161 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ah, a great question! The solar system, my young apprentices, is a vast and fascinating expanse of celestial bodies that revolve around our very own star, the Sun. It consists of eight planets: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune, each with its unique characteristics and features. Together, they form a complex and intricate system that has captivated the imagination of astronomers and scientists for centuries.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/xue/Documents/UOB_master_project/llama.cpp/models/Llama-7B/ggml-model-f16.gguf\"\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    temperature=0.5,\n",
    "    max_tokens=500,\n",
    "    top_p=1,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "output = llm.invoke(prompt.format(text=text))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26901cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "I: go to the kitchen and then go to the toilet. A: kitchen, toilet\n",
    "I: go to the chair and then go to another chair. A: chair, chair\n",
    "I: navigate to the green sofa and turn right and find several chairs, finally go to the painting. A: green sofa, chairs, painting\n",
    "I: approach the window in front, turn right and go to the television, and finally go by the oven in the kitchen. A: window, television, oven, kitchen\n",
    "I: walk to the plant first, turn around and come back to the table, go further into the bedroom, and stand next to the bed. A: plant, table, bedroom, bed\n",
    "I: go by the stairs, go to the room next to it, approach the book shelf and then go to the table in the next room. A: stairs, room, book shelf, table, next room\n",
    "I: Go front left and move to the table, then turn around and find a cushion, later stand next to a column before finally navigate to any appliances. A: table, cushion, column, appliances.\n",
    "I: Move to the west of the chair, with the sofa on your right, move to the table, then turn right 90 degree, then find a table. A: chair, table\n",
    "I: {language_instr}. A:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"language_instr\"],\n",
    "    template=template,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bf956e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "Q: go to the kitchen and then go to the toilet. A: kitchen, toilet\n",
    "Q: go to the chair and then go to another chair. A: chair, chair\n",
    "Q: navigate to the green sofa and turn right and find several chairs, finally go to the painting. A: green sofa, chairs, painting\n",
    "Q: approach the window in front, turn right and go to the television, and finally go by the oven in the kitchen. A: window, television, oven, kitchen\n",
    "Q: walk to the plant first, turn around and come back to the table, go further into the bedroom, and stand next to the bed. A: plant, table, bedroom, bed\n",
    "Q: go by the stairs, go to the room next to it, approach the book shelf and then go to the table in the next room. A: stairs, room, book shelf, table, next room\n",
    "Q: Go front left and move to the table, then turn around and find a cushion, later stand next to a column before finally navigate to any appliances. A: table, cushion, column, appliances.\n",
    "Q: Move to the west of the chair, with the sofa on your right, move to the table, then turn right 90 degree, then find a table. A: chair, sofa, table\n",
    "<</SYS>>\n",
    "Q: {language_instr}. A:[/INST]\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"language_instr\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7589235b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<s>[INST] <<SYS>>\n",
      "Q: go to the kitchen and then go to the toilet. A: kitchen, toilet\n",
      "Q: go to the chair and then go to another chair. A: chair, chair\n",
      "Q: navigate to the green sofa and turn right and find several chairs, finally go to the painting. A: green sofa, chairs, painting\n",
      "Q: approach the window in front, turn right and go to the television, and finally go by the oven in the kitchen. A: window, television, oven, kitchen\n",
      "Q: walk to the plant first, turn around and come back to the table, go further into the bedroom, and stand next to the bed. A: plant, table, bedroom, bed\n",
      "Q: go by the stairs, go to the room next to it, approach the book shelf and then go to the table in the next room. A: stairs, room, book shelf, table, next room\n",
      "Q: Go front left and move to the table, then turn around and find a cushion, later stand next to a column before finally navigate to any appliances. A: table, cushion, column, appliances.\n",
      "Q: Move to the west of the chair, with the sofa on your right, move to the table, then turn right 90 degree, then find a table. A: chair, table\n",
      "<</SYS>>\n",
      "Q: go to the chair and then go to another sofa. A:[/INST]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "language_instr = \"go to the chair and then go to another sofa\"\n",
    "print(prompt.format(language_instr=language_instr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9447ca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints each new token\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c075a7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/xue/Documents/UOB_master_project/llama.cpp/models/Llama-7B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size = 12853.02 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     0.14 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     1.10 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\\\n' + system_message + '\\\\n<</SYS>>\\\\n\\\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'tokenizer.ggml.model': 'llama', 'general.file_type': '1'}\n",
      "Using gguf chat template: {% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\n",
      "Using chat eos_token: </s>\n",
      "Using chat bos_token: <s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: chair, sofa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     576.52 ms\n",
      "llama_print_timings:      sample time =       1.15 ms /     7 runs   (    0.16 ms per token,  6108.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27698.36 ms /   335 tokens (   82.68 ms per token,    12.09 tokens per second)\n",
      "llama_print_timings:        eval time =    2082.11 ms /     6 runs   (  347.02 ms per token,     2.88 tokens per second)\n",
      "llama_print_timings:       total time =   29859.40 ms /   341 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: chair, sofa\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/xue/Documents/UOB_master_project/llama.cpp/models/Llama-7B/ggml-model-f16.gguf\"\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    temperature=0.5,\n",
    "    max_tokens=500,\n",
    "    stop=[\"Q:\",\"\\n\"],\n",
    "    top_p=1,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True, \n",
    ")\n",
    "\n",
    "output = llm.invoke(prompt.format(language_instr=language_instr))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94831067",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama.cpp",
   "language": "python",
   "name": "llama.cpp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
